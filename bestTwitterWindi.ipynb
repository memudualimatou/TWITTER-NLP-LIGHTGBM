{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lightgbm as lgbm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('twitter_train.csv')\n",
    "test=pd.read_csv('twitter_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train_0</td>\n",
       "      <td>The bitcoin halving is cancelled due to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>train_1</td>\n",
       "      <td>MercyOfAllah In good times wrapped in its gran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>train_2</td>\n",
       "      <td>266 Days No Digital India No Murder of e learn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>train_3</td>\n",
       "      <td>India is likely to run out of the remaining RN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>train_4</td>\n",
       "      <td>In these tough times the best way to grow is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               text  target\n",
       "0  train_0            The bitcoin halving is cancelled due to       1\n",
       "1  train_1  MercyOfAllah In good times wrapped in its gran...       0\n",
       "2  train_2  266 Days No Digital India No Murder of e learn...       1\n",
       "3  train_3  India is likely to run out of the remaining RN...       1\n",
       "4  train_4  In these tough times the best way to grow is t...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>test_2</td>\n",
       "      <td>Why is  explained in the video take a look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>test_3</td>\n",
       "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test_4</td>\n",
       "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>test_8</td>\n",
       "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>test_9</td>\n",
       "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               text\n",
       "0  test_2         Why is  explained in the video take a look\n",
       "1  test_3            Ed Davey fasting for Ramadan No contest\n",
       "2  test_4   Is Doja Cat good or do you just miss Nicki Minaj\n",
       "3  test_8  How Boris Johnson s cheery wounded in action p...\n",
       "4  test_9  Man it s terrible Not even a reason to get on ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEROFRMING DATA CLEANING, word segmentation AND STREMMING\n",
    "corpus2 = []\n",
    "for i in range(0, len(train['text'])):\n",
    "    new_test2 = re.sub('[^a-zA-Z]', ' ', train['text'][i])\n",
    "    new_test2 = new_test2.lower()\n",
    "    new_test2 = new_test2.split()\n",
    "    ps = PorterStemmer()\n",
    "    new_test2 = [ps.stem(word) for word in new_test2 if not word in set(stopwords.words('english'))]\n",
    "    new_test2 = ' '.join(new_test2)\n",
    "    corpus2.append(new_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing tokenization\n",
    "wordfreq = {}\n",
    "for sentence in corpus2:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "most_freq = heapq.nlargest(3000, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a bag of words from the most frequent words in the dataset\n",
    "sentence_vectors = []\n",
    "for sentence in corpus2:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " sentence_vectors = np.asarray(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sentence_vectors \n",
    "y = train.iloc[:, 1].values\n",
    "# X=np.clip(X,0.009,0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import re\n",
    "\n",
    "params = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.01,\n",
    "    'num_leaves' : 70,\n",
    "    'feature_fraction': 0.6, \n",
    "    'bagging_fraction': 0.9, \n",
    "    'bagging_freq':0,\n",
    "    'max_depth':-1,\n",
    "    'min_data_in_leaf':0,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric': 'binary_logloss'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 2000 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.301349\n",
      "[400]\tvalid_0's binary_logloss: 0.229903\n",
      "[600]\tvalid_0's binary_logloss: 0.210505\n",
      "[800]\tvalid_0's binary_logloss: 0.207114\n",
      "[1000]\tvalid_0's binary_logloss: 0.209078\n",
      "[1200]\tvalid_0's binary_logloss: 0.213281\n",
      "[1400]\tvalid_0's binary_logloss: 0.220292\n",
      "[1600]\tvalid_0's binary_logloss: 0.228345\n",
      "[1800]\tvalid_0's binary_logloss: 0.236744\n",
      "[2000]\tvalid_0's binary_logloss: 0.245456\n",
      "[2200]\tvalid_0's binary_logloss: 0.254862\n",
      "[2400]\tvalid_0's binary_logloss: 0.264231\n",
      "[2600]\tvalid_0's binary_logloss: 0.274202\n",
      "Early stopping, best iteration is:\n",
      "[756]\tvalid_0's binary_logloss: 0.206826\n",
      "Fininshed Training\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.model_selection import train_test_split\n",
    "    # making test - valid sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "    \n",
    "    # making lgbm datasets for train and valid\n",
    "    d_train = lgbm.Dataset(X_train, Y_train)\n",
    "    d_valid = lgbm.Dataset(X_test, Y_test)\n",
    "    \n",
    "    # training with early stop\n",
    "    bst = lgbm.train(params, d_train, 5000, valid_sets=[d_valid], verbose_eval=200, early_stopping_rounds=2000)\n",
    "    \n",
    "    # making prediciton for one column\n",
    "\n",
    "    y_pred = bst.predict(X_test)\n",
    "#     y_pred=np.clip(y_pred,0.009,0.99)\n",
    "print('Fininshed Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>test_2</td>\n",
       "      <td>Why is  explained in the video take a look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>test_3</td>\n",
       "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test_4</td>\n",
       "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>test_8</td>\n",
       "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>test_9</td>\n",
       "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1957</td>\n",
       "      <td>test_2932</td>\n",
       "      <td>Fageeru meehaa geyga Bandah PUBLIC fundS amp G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1958</td>\n",
       "      <td>test_2934</td>\n",
       "      <td>DFFN Diffusion Pharmaceuticals Announces Pre I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1959</td>\n",
       "      <td>test_2936</td>\n",
       "      <td>I want to wish the Muslim members of Congress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>test_2937</td>\n",
       "      <td>You mean you don t believe there is a conspira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1961</td>\n",
       "      <td>test_2940</td>\n",
       "      <td>Rajavi We call on the United Nations and the S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1962 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               text\n",
       "0        test_2         Why is  explained in the video take a look\n",
       "1        test_3            Ed Davey fasting for Ramadan No contest\n",
       "2        test_4   Is Doja Cat good or do you just miss Nicki Minaj\n",
       "3        test_8  How Boris Johnson s cheery wounded in action p...\n",
       "4        test_9  Man it s terrible Not even a reason to get on ...\n",
       "...         ...                                                ...\n",
       "1957  test_2932  Fageeru meehaa geyga Bandah PUBLIC fundS amp G...\n",
       "1958  test_2934  DFFN Diffusion Pharmaceuticals Announces Pre I...\n",
       "1959  test_2936  I want to wish the Muslim members of Congress ...\n",
       "1960  test_2937  You mean you don t believe there is a conspira...\n",
       "1961  test_2940  Rajavi We call on the United Nations and the S...\n",
       "\n",
       "[1962 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_test= test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus3 = []\n",
    "for i in range(0, len(test['text'])):\n",
    "    new_test3 = re.sub('[^a-zA-Z]', ' ', test['text'][i])\n",
    "    new_test3 = new_test3.lower()\n",
    "    new_test3 = new_test3.split()\n",
    "    ps = PorterStemmer()\n",
    "    new_test3 = [ps.stem(word) for word in new_test3 if not word in set(stopwords.words('english'))]\n",
    "    new_test3 = ' '.join(new_test3)\n",
    "    corpus3.append(new_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq2 = {}\n",
    "for sentence in corpus3:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq2.keys():\n",
    "            wordfreq2[token] = 1\n",
    "        else:\n",
    "            wordfreq2[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "most_freq2 = heapq.nlargest(3500, wordfreq2, key=wordfreq2.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_freq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors2 = []\n",
    "for sentence in corpus3:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors2.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors2 = np.asarray(sentence_vectors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=sentence_vectors2\n",
    "\n",
    "# X2=np.clip(X2,0.1,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_target = bst.predict(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_Twitter_zindi = pd.DataFrame({\"ID\":ID_test,\n",
    "    \"target\":Final_target })\n",
    "submission_Twitter_zindi.to_csv(\"subTwitter_z1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
